{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, default_data_collator\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import evaluate\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завиксируем всю случайность!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 23\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.determenistic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем наш датасет из полученных файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88984, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('anno_first.csv')\n",
    "df2 = pd.read_csv('anno_second.csv')\n",
    "df3 = pd.read_csv('anno_basketball.csv')\n",
    "df4 = pd.read_csv('anno_fiba.csv')\n",
    "#df5 = pd.read_csv('anno_ncaa.csv')\n",
    "df = pd.concat([df1, df2, df4], axis=0)\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем датасет на train test и eval части"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df, test_df = train_test_split(df, test_size=0.2)\n",
    "train_df, eval_df = train_test_split(diff_df, test_size=0.2)\n",
    "# we reset the indices to start from zero\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "eval_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IAMDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=3):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      \n",
    "        file_name = self.df['file_name'][idx]\n",
    "        text = str(self.df['text'][idx])\n",
    "   \n",
    "        image = Image.open(file_name).convert(\"RGB\")\n",
    "        image = image.resize((64, 64))\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        # add labels (input_ids) by encoding the text\n",
    "        labels = self.processor.tokenizer(text, \n",
    "                                          padding=\"max_length\", \n",
    "                                          max_length=self.max_target_length).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим наши датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "train_dataset = IAMDataset(root_dir='C:\\\\Users\\\\Mytre\\\\OneDrive\\\\Документы\\\\Data\\\\Work\\\\',\n",
    "                           df=train_df,\n",
    "                           processor=processor)\n",
    "test_dataset = IAMDataset(root_dir='C:\\\\Users\\\\Mytre\\\\OneDrive\\\\Документы\\\\Data\\\\Work\\\\',\n",
    "                           df=test_df,\n",
    "                           processor=processor)\n",
    "eval_dataset = IAMDataset(root_dir='C:\\\\Users\\\\Mytre\\\\OneDrive\\\\Документы\\\\Data\\\\Work\\\\',\n",
    "                           df=test_df,\n",
    "                           processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 56949\n",
      "Number of testing examples: 17797\n",
      "Number of validation examples: 17797\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of testing examples:\", len(test_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=3)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tensor_board = SummaryWriter()\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сконфигурируем нашу модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 4\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оопределим метрику"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "cer_metric = evaluate.load(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(pred_ids, label_ids):\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    x = [] \n",
    "    for j in pred_str:       \n",
    "        if j.isdigit():\n",
    "            x.append(int(j))\n",
    "        else:\n",
    "            x.append(1000)\n",
    "    acc = acc_metric.compute(predictions=x, references=label_str)\n",
    "\n",
    "    return acc\n",
    "\n",
    "def compute_cer(pred_ids, label_ids):\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 start.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7334daa274547e6b228698e5b1a088d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18983 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690cb1b2f672440cb85d093fc2a3c5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5933 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\transformers\\generation\\utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.12871287128712872\n",
      "Validation cer:: 0.8485148514851485\n",
      "Validation accuracy: 0.16999999999999996\n",
      "Validation cer:: 0.8269485903814262\n",
      "Validation accuracy: 0.12666666666666668\n",
      "Validation cer:: 0.8391472868217055\n",
      "Validation accuracy: 0.16666666666666663\n",
      "Validation cer:: 0.8305664410402565\n",
      "Validation accuracy: 0.14666666666666667\n",
      "Validation cer:: 0.8330149225358806\n",
      "Validation accuracy: 0.13333333333333333\n",
      "Validation cer:: 0.8355914745265828\n",
      "Validation accuracy: 0.11333333333333333\n",
      "Validation cer:: 0.8400720059778548\n",
      "Validation accuracy: 0.12\n",
      "Validation cer:: 0.8430800784733369\n",
      "Validation accuracy: 0.14666666666666664\n",
      "Validation cer:: 0.842460757888061\n",
      "Validation accuracy: 0.11333333333333333\n",
      "Validation cer:: 0.8447290804433661\n",
      "Validation accuracy: 0.16\n",
      "Validation cer:: 0.8421197180052765\n",
      "Validation accuracy: 0.1\n",
      "Validation cer:: 0.8435529915546569\n",
      "Validation accuracy: 0.12666666666666665\n",
      "Validation cer:: 0.844650635042641\n",
      "Validation accuracy: 0.15333333333333335\n",
      "Validation cer:: 0.8444138540498284\n",
      "Validation accuracy: 0.15\n",
      "Validation cer:: 0.8439088226896354\n",
      "Validation accuracy: 0.16\n",
      "Validation cer:: 0.8426548883138515\n",
      "Validation accuracy: 0.13666666666666666\n",
      "Validation cer:: 0.8428417457517988\n",
      "Validation accuracy: 0.13999999999999999\n",
      "Validation cer:: 0.8420639327357817\n",
      "Validation accuracy: 0.13333333333333333\n",
      "Validation cer:: 0.8420342676786653\n",
      "Validation accuracy: 0.13333333333333333\n",
      "Validation cer:: 0.8427322053259085\n",
      "Validation accuracy: 0.12\n",
      "Validation cer:: 0.8438634663765554\n",
      "Validation accuracy: 0.12\n",
      "Validation cer:: 0.8446950520326258\n",
      "Validation accuracy: 0.14666666666666667\n",
      "Validation cer:: 0.8445127377330767\n",
      "Validation accuracy: 0.11666666666666665\n",
      "Validation cer:: 0.8449634081037662\n",
      "Validation accuracy: 0.16\n",
      "Validation cer:: 0.8437386949981912\n",
      "Validation accuracy: 0.14\n",
      "Validation cer:: 0.8434155361491001\n",
      "Validation accuracy: 0.12333333333333334\n",
      "Validation cer:: 0.8434803688228346\n",
      "Validation accuracy: 0.13\n",
      "Validation cer:: 0.8438321347817955\n",
      "Validation accuracy: 0.16666666666666663\n",
      "Validation cer:: 0.8428497562416901\n",
      "Validation accuracy: 0.11\n",
      "Validation cer:: 0.8434204471525364\n",
      "Validation accuracy: 0.15333333333333335\n",
      "Validation cer:: 0.8426920655395341\n",
      "Validation accuracy: 0.11333333333333334\n",
      "Validation cer:: 0.8433993841210335\n",
      "Validation accuracy: 0.14666666666666664\n",
      "Validation cer:: 0.8424582738275558\n",
      "Validation accuracy: 0.1533333333333333\n",
      "Validation cer:: 0.8418665378530125\n",
      "Validation accuracy: 0.11000000000000001\n",
      "Validation cer:: 0.8425558683913441\n",
      "Validation accuracy: 0.12333333333333334\n",
      "Validation cer:: 0.8429847529125508\n",
      "Validation accuracy: 0.13333333333333333\n",
      "Validation cer:: 0.8431427799436446\n",
      "Validation accuracy: 0.12666666666666665\n",
      "Validation cer:: 0.8432223349745054\n",
      "Validation accuracy: 0.14333333333333334\n",
      "Validation cer:: 0.8432593596269576\n",
      "Validation accuracy: 0.13\n",
      "Validation cer:: 0.8433986741409886\n",
      "Validation accuracy: 0.12333333333333334\n",
      "Validation cer:: 0.8436693721624225\n",
      "Validation accuracy: 0.12666666666666665\n",
      "Validation cer:: 0.8441255483388309\n",
      "Validation accuracy: 0.12333333333333334\n",
      "Validation cer:: 0.8440412528647823\n",
      "Validation accuracy: 0.1366666666666667\n",
      "Validation cer:: 0.8437827982817758\n",
      "Validation accuracy: 0.13666666666666666\n",
      "Validation cer:: 0.8438135440801515\n",
      "Validation accuracy: 0.09333333333333332\n",
      "Validation cer:: 0.8445239647695636\n",
      "Validation accuracy: 0.10666666666666667\n",
      "Validation cer:: 0.8449134429351405\n",
      "Validation accuracy: 0.12000000000000002\n",
      "Validation cer:: 0.8450436912944724\n",
      "Validation accuracy: 0.09333333333333334\n",
      "Validation cer:: 0.8456651217924428\n",
      "Validation accuracy: 0.15333333333333335\n",
      "Validation cer:: 0.8452718980013518\n",
      "Validation accuracy: 0.13\n",
      "Validation cer:: 0.8452796370459573\n",
      "Validation accuracy: 0.12666666666666665\n",
      "Validation cer:: 0.8452582378846559\n",
      "Validation accuracy: 0.15666666666666665\n",
      "Validation cer:: 0.8450867311648295\n",
      "Validation accuracy: 0.13333333333333333\n",
      "Validation cer:: 0.8449616913975366\n",
      "Validation accuracy: 0.15666666666666665\n",
      "Validation cer:: 0.8448290786956484\n",
      "Validation accuracy: 0.12\n",
      "Validation cer:: 0.84513862320504\n",
      "Validation accuracy: 0.15333333333333332\n",
      "Validation cer:: 0.8450660285163004\n",
      "Validation accuracy: 0.10666666666666667\n",
      "Validation cer:: 0.8455935347764343\n",
      "Validation accuracy: 0.13\n",
      "Validation cer:: 0.8456113168873716\n",
      "Validation accuracy: 0.13292881622563066\n",
      "Epoch 1 start.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e07babf6bc481191d368d056a47e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18983 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1126067fef48d6bd40e5071dab7ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5933 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.12871287128712872\n",
      "Validation cer:: 0.8485148514851485\n",
      "Validation accuracy: 0.16999999999999996\n",
      "Validation cer:: 0.8269485903814262\n",
      "Validation accuracy: 0.12666666666666668\n",
      "Validation cer:: 0.8391472868217055\n",
      "Validation accuracy: 0.16666666666666663\n",
      "Validation cer:: 0.8305664410402565\n",
      "Validation accuracy: 0.14666666666666667\n",
      "Validation cer:: 0.8330149225358806\n",
      "Validation accuracy: 0.13333333333333333\n",
      "Validation cer:: 0.8355914745265828\n",
      "Validation accuracy: 0.11333333333333333\n",
      "Validation cer:: 0.8400720059778548\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     52\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(eval_dataloader):\n\u001b[0;32m     53\u001b[0m         \u001b[39m# run batch generation\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m         outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(batch[\u001b[39m\"\u001b[39;49m\u001b[39mpixel_values\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device), max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n\u001b[0;32m     55\u001b[0m         \u001b[39m# compute metrics\u001b[39;00m\n\u001b[0;32m     57\u001b[0m         acc \u001b[39m=\u001b[39m compute_acc(pred_ids\u001b[39m=\u001b[39moutputs, label_ids\u001b[39m=\u001b[39mbatch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\transformers\\generation\\utils.py:1490\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[0;32m   1483\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1484\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1485\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   1486\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1487\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1488\u001b[0m     )\n\u001b[0;32m   1489\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[1;32m-> 1490\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeam_search(\n\u001b[0;32m   1491\u001b[0m         input_ids,\n\u001b[0;32m   1492\u001b[0m         beam_scorer,\n\u001b[0;32m   1493\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1494\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1495\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1496\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1497\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1498\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1499\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1500\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1501\u001b[0m     )\n\u001b[0;32m   1503\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[0;32m   1504\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\transformers\\generation\\utils.py:2768\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   2763\u001b[0m next_token_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madjust_logits_during_generation(next_token_logits, cur_len\u001b[39m=\u001b[39mcur_len)\n\u001b[0;32m   2764\u001b[0m next_token_scores \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mlog_softmax(\n\u001b[0;32m   2765\u001b[0m     next_token_logits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m   2766\u001b[0m )  \u001b[39m# (batch_size * num_beams, vocab_size)\u001b[39;00m\n\u001b[1;32m-> 2768\u001b[0m next_token_scores_processed \u001b[39m=\u001b[39m logits_processor(input_ids, next_token_scores)\n\u001b[0;32m   2769\u001b[0m next_token_scores \u001b[39m=\u001b[39m next_token_scores_processed \u001b[39m+\u001b[39m beam_scores[:, \u001b[39mNone\u001b[39;00m]\u001b[39m.\u001b[39mexpand_as(next_token_scores)\n\u001b[0;32m   2771\u001b[0m \u001b[39m# Store scores, attentions and hidden_states when required\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\transformers\\generation\\logits_process.py:92\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[1;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m         scores \u001b[39m=\u001b[39m processor(input_ids, scores, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     91\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m         scores \u001b[39m=\u001b[39m processor(input_ids, scores)\n\u001b[0;32m     93\u001b[0m \u001b[39mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\transformers\\generation\\logits_process.py:487\u001b[0m, in \u001b[0;36mNoRepeatNGramLogitsProcessor.__call__\u001b[1;34m(self, input_ids, scores)\u001b[0m\n\u001b[0;32m    485\u001b[0m num_batch_hypotheses \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    486\u001b[0m cur_len \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m--> 487\u001b[0m banned_batch_tokens \u001b[39m=\u001b[39m _calc_banned_ngram_tokens(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mngram_size, input_ids, num_batch_hypotheses, cur_len)\n\u001b[0;32m    489\u001b[0m \u001b[39mfor\u001b[39;00m i, banned_tokens \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(banned_batch_tokens):\n\u001b[0;32m    490\u001b[0m     scores[i, banned_tokens] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minf\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\transformers\\generation\\logits_process.py:460\u001b[0m, in \u001b[0;36m_calc_banned_ngram_tokens\u001b[1;34m(ngram_size, prev_input_ids, num_hypos, cur_len)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39mif\u001b[39;00m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m<\u001b[39m ngram_size:\n\u001b[0;32m    457\u001b[0m     \u001b[39m# return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\u001b[39;00m\n\u001b[0;32m    458\u001b[0m     \u001b[39mreturn\u001b[39;00m [[] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_hypos)]\n\u001b[1;32m--> 460\u001b[0m generated_ngrams \u001b[39m=\u001b[39m _get_ngrams(ngram_size, prev_input_ids, num_hypos)\n\u001b[0;32m    462\u001b[0m banned_tokens \u001b[39m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m     _get_generated_ngrams(generated_ngrams[hypo_idx], prev_input_ids[hypo_idx], ngram_size, cur_len)\n\u001b[0;32m    464\u001b[0m     \u001b[39mfor\u001b[39;00m hypo_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_hypos)\n\u001b[0;32m    465\u001b[0m ]\n\u001b[0;32m    466\u001b[0m \u001b[39mreturn\u001b[39;00m banned_tokens\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\transformers\\generation\\logits_process.py:437\u001b[0m, in \u001b[0;36m_get_ngrams\u001b[1;34m(ngram_size, prev_input_ids, num_hypos)\u001b[0m\n\u001b[0;32m    435\u001b[0m generated_ngrams \u001b[39m=\u001b[39m [{} \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_hypos)]\n\u001b[0;32m    436\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_hypos):\n\u001b[1;32m--> 437\u001b[0m     gen_tokens \u001b[39m=\u001b[39m prev_input_ids[idx]\u001b[39m.\u001b[39;49mtolist()\n\u001b[0;32m    438\u001b[0m     generated_ngram \u001b[39m=\u001b[39m generated_ngrams[idx]\n\u001b[0;32m    439\u001b[0m     \u001b[39mfor\u001b[39;00m ngram \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[gen_tokens[i:] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ngram_size)]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "step_val = 0\n",
    "accuracy = 0.\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    acc_batch = []\n",
    "    train_loss_batch = []\n",
    "    i, j = 0, 0\n",
    "    \n",
    "    print(f'Epoch {epoch} start.')\n",
    "\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # get the inputs\n",
    "        for k,v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "        break\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #acc_train = compute_acc(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
    "        #acc_batch.append(acc_train['accuracy'])\n",
    "        train_loss.append(loss.item())\n",
    "        train_loss_batch.append(loss.item())\n",
    "\n",
    "        if (i % 100 == 0) & (i > 2 ):\n",
    "            print(f'Train loss: {np.mean(train_loss_batch)}')\n",
    "            tensor_board.add_scalar('Train loss:', np.mean(train_loss_batch), global_step=step)   \n",
    "            #tensor_board.add_scalar('Validation accuracy:', np.mean(acc_batch), global_step=step_val)   \n",
    "            #acc_batch = []      \n",
    "            step += 1\n",
    "        i += 1\n",
    "        \n",
    "    print(f\"Loss after epoch {epoch}:\", np.mean(train_loss))\n",
    "    \n",
    "    # evaluate\n",
    "    model.eval()\n",
    "\n",
    "    valid_acc = []\n",
    "    valid_cer = []\n",
    "    valid_acc_batch = []\n",
    "    valid_cer_batch = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_dataloader):\n",
    "            # run batch generation\n",
    "            outputs = model.generate(batch[\"pixel_values\"].to(device), max_new_tokens=4)\n",
    "            # compute metrics\n",
    "            \n",
    "            acc = compute_acc(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
    "            cer = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])           \n",
    "            \n",
    "            valid_cer.append(cer)       \n",
    "            valid_acc.append(acc['accuracy'])  \n",
    "            valid_acc_batch.append(acc['accuracy']) \n",
    "            valid_cer_batch.append(cer)\n",
    "\n",
    "            if (j % 100 == 0) & (j > 2 ):\n",
    "                print(f'Validation accuracy: {np.mean(valid_acc_batch)}')\n",
    "                print(f'Validation cer:: {np.mean(valid_cer_batch)}')\n",
    "\n",
    "                tensor_board.add_scalar('Validation accuracy:', np.mean(valid_acc_batch), global_step=step_val)\n",
    "                tensor_board.add_scalar('Validation cer:', np.mean(valid_cer_batch), global_step=step_val)\n",
    "                \n",
    "                valid_acc_batch = []\n",
    "                valid_cer_batch\n",
    "                step_val += 1\n",
    "            j += 1 \n",
    "\n",
    "        counter += 1    \n",
    "        if np.mean(valid_acc) > accuracy:\n",
    "            model.save_pretrained('model_best')    \n",
    "            accuracy = np.mean(valid_acc)\n",
    "            counter = 0\n",
    "\n",
    "        if counter > 5:\n",
    "            print('Early stopping!!!')\n",
    "            print(f'Result validation accuracy: {accuracy}')\n",
    "            break\n",
    "\n",
    "        print(\"Validation accuracy:\", np.mean(valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
