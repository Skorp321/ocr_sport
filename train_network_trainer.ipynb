{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, default_data_collator\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завиксируем всю случайность!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 23\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.determenistic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем наш датасет из полученных файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88984, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('anno_first.csv')\n",
    "df2 = pd.read_csv('anno_second.csv')\n",
    "df3 = pd.read_csv('anno_basketball.csv')\n",
    "df4 = pd.read_csv('anno_fiba.csv')\n",
    "#df5 = pd.read_csv('anno_ncaa.csv')\n",
    "df = pd.concat([df1, df2, df4], axis=0)\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем датасет на train test и eval части"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df, eval_df = train_test_split(df, test_size=0.2)\n",
    "train_df, test_df = train_test_split(diff_df, test_size=0.2)\n",
    "# we reset the indices to start from zero\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "eval_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_pathes = []\n",
    "\n",
    "class IAMDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=3):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      \n",
    "        file_name = self.df['file_name'][idx]\n",
    "        text = str(self.df['text'][idx])\n",
    "        store_pathes.append(file_name)\n",
    "   \n",
    "        image = Image.open(file_name).convert(\"RGB\")\n",
    "        image = image.resize((64, 64))\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        # add labels (input_ids) by encoding the text\n",
    "        labels = self.processor.tokenizer(text, \n",
    "                                          padding=\"max_length\", \n",
    "                                          max_length=self.max_target_length).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим наши датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "train_dataset = IAMDataset(root_dir='C:\\\\Users\\\\Mytre\\\\OneDrive\\\\Документы\\\\Data\\\\Work\\\\',\n",
    "                           df=train_df,\n",
    "                           processor=processor)\n",
    "test_dataset = IAMDataset(root_dir='C:\\\\Users\\\\Mytre\\\\OneDrive\\\\Документы\\\\Data\\\\Work\\\\',\n",
    "                           df=test_df,\n",
    "                           processor=processor)\n",
    "eval_dataset = IAMDataset(root_dir='C:\\\\Users\\\\Mytre\\\\OneDrive\\\\Документы\\\\Data\\\\Work\\\\',\n",
    "                           df=test_df,\n",
    "                           processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 56949\n",
      "Number of testing examples: 14238\n",
      "Number of validation examples: 14238\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of testing examples:\", len(test_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим предъобученный трансформер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сконфигурируем нашу модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 4\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сконфигурируем цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    num_train_epochs=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    fp16=True, \n",
    "    output_dir=\"./\",\n",
    "    logging_steps=4000,\n",
    "    save_steps=40000,\n",
    "    eval_steps=20000,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оопределим метрику"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "cer_metric = evaluate.load(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переопределим оптимизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\transformers\\models\\trocr\\processing_trocr.py:134: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\transformers\\optimization.py:346: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2e51dc709c40c3ac02216978c8a4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/189830 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6694, 'learning_rate': 4.894774271716799e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3131, 'learning_rate': 4.7894695253648e-05, 'epoch': 0.42}\n",
      "{'loss': 0.2273, 'learning_rate': 4.6841384396565354e-05, 'epoch': 0.63}\n",
      "{'loss': 0.1881, 'learning_rate': 4.578860032660802e-05, 'epoch': 0.84}\n",
      "{'loss': 0.1683, 'learning_rate': 4.473528946952537e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\transformers\\generation\\utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125f432ddbd04922b4e197a462d77490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.082074165344238, 'eval_cer': 0.26678765880217786, 'eval_runtime': 2710.7428, 'eval_samples_per_second': 5.252, 'eval_steps_per_second': 1.751, 'epoch': 1.05}\n",
      "{'loss': 0.147, 'learning_rate': 4.3682505399568034e-05, 'epoch': 1.26}\n",
      "{'loss': 0.1419, 'learning_rate': 4.2629457936048044e-05, 'epoch': 1.48}\n",
      "{'loss': 0.1286, 'learning_rate': 4.15761470789654e-05, 'epoch': 1.69}\n",
      "{'loss': 0.133, 'learning_rate': 4.0522836221882736e-05, 'epoch': 1.9}\n",
      "{'loss': 0.1115, 'learning_rate': 3.947005215192541e-05, 'epoch': 2.11}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378160f47fba481f85e87c610a116fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.892834186553955, 'eval_cer': 0.1948363678941514, 'eval_runtime': 2710.2859, 'eval_samples_per_second': 5.253, 'eval_steps_per_second': 1.751, 'epoch': 2.11}\n",
      "{'loss': 0.1052, 'learning_rate': 3.8416741294842757e-05, 'epoch': 2.32}\n",
      "{'loss': 0.0955, 'learning_rate': 3.7363693831322767e-05, 'epoch': 2.53}\n",
      "{'loss': 0.0953, 'learning_rate': 3.6310646367802777e-05, 'epoch': 2.74}\n",
      "{'loss': 0.0945, 'learning_rate': 3.525707211715746e-05, 'epoch': 2.95}\n",
      "{'loss': 0.0756, 'learning_rate': 3.420402465363747e-05, 'epoch': 3.16}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8013385aebee46899e1dab303dc4cb5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.505166053771973, 'eval_cer': 0.3093495696973245, 'eval_runtime': 2750.3964, 'eval_samples_per_second': 5.177, 'eval_steps_per_second': 1.726, 'epoch': 3.16}\n",
      "{'loss': 0.0792, 'learning_rate': 3.3151240583680136e-05, 'epoch': 3.37}\n",
      "{'loss': 0.0771, 'learning_rate': 3.209766633303482e-05, 'epoch': 3.58}\n",
      "{'loss': 0.0776, 'learning_rate': 3.104435547595217e-05, 'epoch': 3.79}\n",
      "{'loss': 0.0756, 'learning_rate': 2.9991308012432178e-05, 'epoch': 4.0}\n",
      "{'loss': 0.0563, 'learning_rate': 2.8937733761786863e-05, 'epoch': 4.21}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66cff606d7774797beab9c5834094d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.794046401977539, 'eval_cer': 0.11978221415607986, 'eval_runtime': 2690.793, 'eval_samples_per_second': 5.291, 'eval_steps_per_second': 1.764, 'epoch': 4.21}\n",
      "{'loss': 0.0581, 'learning_rate': 2.788468629826687e-05, 'epoch': 4.43}\n",
      "{'loss': 0.0609, 'learning_rate': 2.683137544118422e-05, 'epoch': 4.64}\n",
      "{'loss': 0.0608, 'learning_rate': 2.577832797766423e-05, 'epoch': 4.85}\n",
      "{'loss': 0.0492, 'learning_rate': 2.4725017120581572e-05, 'epoch': 5.06}\n",
      "{'loss': 0.0438, 'learning_rate': 2.367170626349892e-05, 'epoch': 5.27}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59bab49814674fe2b9bcd62f2b48fcf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.273472785949707, 'eval_cer': 0.229436215678239, 'eval_runtime': 2683.3841, 'eval_samples_per_second': 5.306, 'eval_steps_per_second': 1.769, 'epoch': 5.27}\n",
      "{'loss': 0.0448, 'learning_rate': 2.2618922193541592e-05, 'epoch': 5.48}\n",
      "{'loss': 0.0452, 'learning_rate': 2.1565347942896277e-05, 'epoch': 5.69}\n",
      "{'loss': 0.0451, 'learning_rate': 2.0511773692250963e-05, 'epoch': 5.9}\n",
      "{'loss': 0.0393, 'learning_rate': 1.9458989622293634e-05, 'epoch': 6.11}\n",
      "{'loss': 0.0317, 'learning_rate': 1.840567876521098e-05, 'epoch': 6.32}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4daa354f36524d06aa229eba6dd8bd96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.03128719329834, 'eval_cer': 0.3169603653181898, 'eval_runtime': 2671.6253, 'eval_samples_per_second': 5.329, 'eval_steps_per_second': 1.776, 'epoch': 6.32}\n",
      "{'loss': 0.0309, 'learning_rate': 1.7352367908128326e-05, 'epoch': 6.53}\n",
      "{'loss': 0.0297, 'learning_rate': 1.6299057051045675e-05, 'epoch': 6.74}\n",
      "{'loss': 0.0348, 'learning_rate': 1.524600958752568e-05, 'epoch': 6.95}\n",
      "{'loss': 0.0206, 'learning_rate': 1.4192435336880367e-05, 'epoch': 7.16}\n",
      "{'loss': 0.0236, 'learning_rate': 1.3139651266923036e-05, 'epoch': 7.38}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d009bf66284083ac79b279de576072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.167418479919434, 'eval_cer': 0.16017797552836485, 'eval_runtime': 2691.8952, 'eval_samples_per_second': 5.289, 'eval_steps_per_second': 1.763, 'epoch': 7.38}\n",
      "{'loss': 0.0197, 'learning_rate': 1.2086340409840384e-05, 'epoch': 7.59}\n",
      "{'loss': 0.0224, 'learning_rate': 1.1033292946320392e-05, 'epoch': 7.8}\n",
      "{'loss': 0.019, 'learning_rate': 9.97998208923774e-06, 'epoch': 8.01}\n",
      "{'loss': 0.0133, 'learning_rate': 8.926934625717748e-06, 'epoch': 8.22}\n",
      "{'loss': 0.0159, 'learning_rate': 7.873623768635094e-06, 'epoch': 8.43}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59c581c756e49d3ac0aba05db10ed2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.0792388916015625, 'eval_cer': 0.1537966161231778, 'eval_runtime': 2706.3699, 'eval_samples_per_second': 5.261, 'eval_steps_per_second': 1.754, 'epoch': 8.43}\n",
      "{'loss': 0.0156, 'learning_rate': 6.820312911552441e-06, 'epoch': 8.64}\n",
      "{'loss': 0.0107, 'learning_rate': 5.767002054469789e-06, 'epoch': 8.85}\n",
      "{'loss': 0.0109, 'learning_rate': 4.7134278038244746e-06, 'epoch': 9.06}\n",
      "{'loss': 0.0093, 'learning_rate': 3.660116946741822e-06, 'epoch': 9.27}\n",
      "{'loss': 0.0095, 'learning_rate': 2.6070694832218302e-06, 'epoch': 9.48}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67d5524763943fa81b4efee16fea976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.954314231872559, 'eval_cer': 0.11480592471166794, 'eval_runtime': 2692.7972, 'eval_samples_per_second': 5.287, 'eval_steps_per_second': 1.762, 'epoch': 9.48}\n",
      "{'loss': 0.0096, 'learning_rate': 1.5540220197018386e-06, 'epoch': 9.69}\n",
      "{'loss': 0.0044, 'learning_rate': 5.007111626191856e-07, 'epoch': 9.9}\n",
      "{'train_runtime': 102249.0808, 'train_samples_per_second': 5.57, 'train_steps_per_second': 1.857, 'train_loss': 0.0809435841758755, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Номер на футболке: 8\n"
     ]
    }
   ],
   "source": [
    "url = \"crops\\\\8\\\\ballerTV_137example.jpg\"\n",
    "image = Image.open(url).convert(\"RGB\")\n",
    "\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "generated_ids = model.generate(pixel_values.cuda())\n",
    "\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(f'Номер на футболке: {generated_text}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим eval_dataloader и Загрузим обученную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = DataLoader(eval_dataset, batch_size=32)\n",
    "model = VisionEncoderDecoderModel.from_pretrained('checkpoint-80000')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем точность обученной модели на eval датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the device to run the evaluation on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "model.to(device)\n",
    "# Evaluate the model on the eval dataset\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in tqdm(enumerate(eval_dataloader)):\n",
    "        #url = eval_df['file_name'][i]\n",
    "        target_text = processor.batch_decode(batch['labels'], skip_special_tokens=True)\n",
    "        #image = Image.open(url).convert(\"RGB\")        \n",
    "        pixel_values = batch['pixel_values']\n",
    "        generated_ids = model.generate(pixel_values.cuda())       \n",
    "\n",
    "        # Make a prediction\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        x = [] \n",
    "        for j in generated_text:       \n",
    "            if j.isdigit():\n",
    "                x.append(int(j))\n",
    "            else:\n",
    "                x.append(1000)        \n",
    "\n",
    "        # Save the true and predicted labels\n",
    "        y_true.append(target_text)\n",
    "        y_pred.append(x)  \n",
    "\n",
    "        if (i % 100  == 0) & (i > 2):\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            print(\"Accuracy:\", accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation examples: 23894\n"
     ]
    }
   ],
   "source": [
    "df10 = pd.read_csv('anno_00new.csv')\n",
    "\n",
    "eval_dataset = IAMDataset(root_dir='C:\\\\Users\\\\Mytre\\\\OneDrive\\\\Документы\\\\Data\\\\Work\\\\',\n",
    "                           df=df10,\n",
    "                           processor=processor)\n",
    "\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем точность обученной модели на eval датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7acb51e8a1aa4898b847e8fb2767563c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23894 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6982543640897756\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(url)\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)   \n\u001b[0;32m     20\u001b[0m pixel_values \u001b[39m=\u001b[39m processor(image, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mpixel_values\n\u001b[1;32m---> 21\u001b[0m generated_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(pixel_values\u001b[39m.\u001b[39;49mcuda(), max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[0;32m     23\u001b[0m generated_text \u001b[39m=\u001b[39m processor\u001b[39m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     25\u001b[0m \u001b[39m#pixel_values = batch['pixel_values']\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m#generated_ids = model.generate(pixel_values.cuda())       \u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[39m# Make a prediction\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m#generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\transformers\\generation\\utils.py:1490\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[0;32m   1483\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1484\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1485\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   1486\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1487\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1488\u001b[0m     )\n\u001b[0;32m   1489\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[1;32m-> 1490\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeam_search(\n\u001b[0;32m   1491\u001b[0m         input_ids,\n\u001b[0;32m   1492\u001b[0m         beam_scorer,\n\u001b[0;32m   1493\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1494\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1495\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1496\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1497\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1498\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1499\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1500\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1501\u001b[0m     )\n\u001b[0;32m   1503\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[0;32m   1504\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\transformers\\generation\\utils.py:2768\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   2763\u001b[0m next_token_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madjust_logits_during_generation(next_token_logits, cur_len\u001b[39m=\u001b[39mcur_len)\n\u001b[0;32m   2764\u001b[0m next_token_scores \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mlog_softmax(\n\u001b[0;32m   2765\u001b[0m     next_token_logits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m   2766\u001b[0m )  \u001b[39m# (batch_size * num_beams, vocab_size)\u001b[39;00m\n\u001b[1;32m-> 2768\u001b[0m next_token_scores_processed \u001b[39m=\u001b[39m logits_processor(input_ids, next_token_scores)\n\u001b[0;32m   2769\u001b[0m next_token_scores \u001b[39m=\u001b[39m next_token_scores_processed \u001b[39m+\u001b[39m beam_scores[:, \u001b[39mNone\u001b[39;00m]\u001b[39m.\u001b[39mexpand_as(next_token_scores)\n\u001b[0;32m   2771\u001b[0m \u001b[39m# Store scores, attentions and hidden_states when required\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\transformers\\generation\\logits_process.py:92\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[1;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m         scores \u001b[39m=\u001b[39m processor(input_ids, scores, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     91\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m         scores \u001b[39m=\u001b[39m processor(input_ids, scores)\n\u001b[0;32m     93\u001b[0m \u001b[39mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\transformers\\generation\\logits_process.py:487\u001b[0m, in \u001b[0;36mNoRepeatNGramLogitsProcessor.__call__\u001b[1;34m(self, input_ids, scores)\u001b[0m\n\u001b[0;32m    485\u001b[0m num_batch_hypotheses \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    486\u001b[0m cur_len \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m--> 487\u001b[0m banned_batch_tokens \u001b[39m=\u001b[39m _calc_banned_ngram_tokens(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mngram_size, input_ids, num_batch_hypotheses, cur_len)\n\u001b[0;32m    489\u001b[0m \u001b[39mfor\u001b[39;00m i, banned_tokens \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(banned_batch_tokens):\n\u001b[0;32m    490\u001b[0m     scores[i, banned_tokens] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minf\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\transformers\\generation\\logits_process.py:460\u001b[0m, in \u001b[0;36m_calc_banned_ngram_tokens\u001b[1;34m(ngram_size, prev_input_ids, num_hypos, cur_len)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39mif\u001b[39;00m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m<\u001b[39m ngram_size:\n\u001b[0;32m    457\u001b[0m     \u001b[39m# return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\u001b[39;00m\n\u001b[0;32m    458\u001b[0m     \u001b[39mreturn\u001b[39;00m [[] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_hypos)]\n\u001b[1;32m--> 460\u001b[0m generated_ngrams \u001b[39m=\u001b[39m _get_ngrams(ngram_size, prev_input_ids, num_hypos)\n\u001b[0;32m    462\u001b[0m banned_tokens \u001b[39m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m     _get_generated_ngrams(generated_ngrams[hypo_idx], prev_input_ids[hypo_idx], ngram_size, cur_len)\n\u001b[0;32m    464\u001b[0m     \u001b[39mfor\u001b[39;00m hypo_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_hypos)\n\u001b[0;32m    465\u001b[0m ]\n\u001b[0;32m    466\u001b[0m \u001b[39mreturn\u001b[39;00m banned_tokens\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\transformers\\generation\\logits_process.py:437\u001b[0m, in \u001b[0;36m_get_ngrams\u001b[1;34m(ngram_size, prev_input_ids, num_hypos)\u001b[0m\n\u001b[0;32m    435\u001b[0m generated_ngrams \u001b[39m=\u001b[39m [{} \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_hypos)]\n\u001b[0;32m    436\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_hypos):\n\u001b[1;32m--> 437\u001b[0m     gen_tokens \u001b[39m=\u001b[39m prev_input_ids[idx]\u001b[39m.\u001b[39;49mtolist()\n\u001b[0;32m    438\u001b[0m     generated_ngram \u001b[39m=\u001b[39m generated_ngrams[idx]\n\u001b[0;32m    439\u001b[0m     \u001b[39mfor\u001b[39;00m ngram \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[gen_tokens[i:] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ngram_size)]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the device to run the evaluation on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "model.to(device)\n",
    "# Evaluate the model on the eval dataset\n",
    "y_true = []\n",
    "y_pred = []\n",
    "xx = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(eval_dataset)), nrows=2):\n",
    "        url = df10['file_name'][i]\n",
    "        xx.append(url)\n",
    "        target_text = int(df10['text'][i])\n",
    "        image = Image.open(url).convert(\"RGB\")   \n",
    "\n",
    "        pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "        generated_ids = model.generate(pixel_values.cuda(), max_new_tokens=3)\n",
    "\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        #pixel_values = batch['pixel_values']\n",
    "        #generated_ids = model.generate(pixel_values.cuda())       \n",
    "\n",
    "        # Make a prediction\n",
    "        #generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        x = []        \n",
    "        if generated_text.isdigit():\n",
    "            \n",
    "            x = int(generated_text)\n",
    "        else:\n",
    "            x = 1000    \n",
    "        \n",
    "        # Save the true and predicted labels\n",
    "        y_true.append(target_text)\n",
    "        y_pred.append(x)\n",
    "\n",
    "        if (i % 400  == 0) & (i > 2):\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            print(\"Accuracy:\", accuracy) \n",
    "\n",
    "print(\"Accuracy:\", accuracy)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
