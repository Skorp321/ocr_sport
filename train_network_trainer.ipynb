{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, default_data_collator\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завиксируем всю случайность!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 23\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.determenistic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем наш датасет из полученных файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88984, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('anno_first.csv')\n",
    "df2 = pd.read_csv('anno_second.csv')\n",
    "df3 = pd.read_csv('anno_basketball.csv')\n",
    "df4 = pd.read_csv('anno_fiba.csv')\n",
    "#df5 = pd.read_csv('anno_ncaa.csv')\n",
    "df = pd.concat([df1, df2, df4], axis=0)\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем датасет на train test и eval части"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df, eval_df = train_test_split(df, test_size=0.2)\n",
    "train_df, test_df = train_test_split(diff_df, test_size=0.2)\n",
    "# we reset the indices to start from zero\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "eval_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_pathes = []\n",
    "\n",
    "class IAMDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=3):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      \n",
    "        file_name = self.df['file_name'][idx]\n",
    "        text = str(self.df['text'][idx])\n",
    "        store_pathes.append(file_name)\n",
    "   \n",
    "        image = Image.open(file_name).convert(\"RGB\")\n",
    "        image = image.resize((64, 64))\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        # add labels (input_ids) by encoding the text\n",
    "        labels = self.processor.tokenizer(text, \n",
    "                                          padding=\"max_length\", \n",
    "                                          max_length=self.max_target_length).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим наши датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "train_dataset = IAMDataset(root_dir='C:\\\\Users\\\\Mytre\\\\OneDrive\\\\Документы\\\\Data\\\\Work\\\\',\n",
    "                           df=train_df,\n",
    "                           processor=processor)\n",
    "test_dataset = IAMDataset(root_dir='C:\\\\Users\\\\Mytre\\\\OneDrive\\\\Документы\\\\Data\\\\Work\\\\',\n",
    "                           df=test_df,\n",
    "                           processor=processor)\n",
    "eval_dataset = IAMDataset(root_dir='C:\\\\Users\\\\Mytre\\\\OneDrive\\\\Документы\\\\Data\\\\Work\\\\',\n",
    "                           df=test_df,\n",
    "                           processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 56949\n",
      "Number of testing examples: 14238\n",
      "Number of validation examples: 14238\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of testing examples:\", len(test_dataset))\n",
    "print(\"Number of validation examples:\", len(test_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим предъобученный трансформер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сконфигурируем нашу модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 4\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сконфигурируем цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    num_train_epochs=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    fp16=True, \n",
    "    output_dir=\"./\",\n",
    "    logging_steps=4000,\n",
    "    save_steps=40000,\n",
    "    eval_steps=20000,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оопределим метрику"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "cer_metric = evaluate.load(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переопределим оптимизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\transformers\\models\\trocr\\processing_trocr.py:134: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\transformers\\optimization.py:346: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2e51dc709c40c3ac02216978c8a4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/189830 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6694, 'learning_rate': 4.894774271716799e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3131, 'learning_rate': 4.7894695253648e-05, 'epoch': 0.42}\n",
      "{'loss': 0.2273, 'learning_rate': 4.6841384396565354e-05, 'epoch': 0.63}\n",
      "{'loss': 0.1881, 'learning_rate': 4.578860032660802e-05, 'epoch': 0.84}\n",
      "{'loss': 0.1683, 'learning_rate': 4.473528946952537e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\workenv\\lib\\site-packages\\transformers\\generation\\utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125f432ddbd04922b4e197a462d77490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.082074165344238, 'eval_cer': 0.26678765880217786, 'eval_runtime': 2710.7428, 'eval_samples_per_second': 5.252, 'eval_steps_per_second': 1.751, 'epoch': 1.05}\n",
      "{'loss': 0.147, 'learning_rate': 4.3682505399568034e-05, 'epoch': 1.26}\n",
      "{'loss': 0.1419, 'learning_rate': 4.2629457936048044e-05, 'epoch': 1.48}\n",
      "{'loss': 0.1286, 'learning_rate': 4.15761470789654e-05, 'epoch': 1.69}\n",
      "{'loss': 0.133, 'learning_rate': 4.0522836221882736e-05, 'epoch': 1.9}\n",
      "{'loss': 0.1115, 'learning_rate': 3.947005215192541e-05, 'epoch': 2.11}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378160f47fba481f85e87c610a116fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.892834186553955, 'eval_cer': 0.1948363678941514, 'eval_runtime': 2710.2859, 'eval_samples_per_second': 5.253, 'eval_steps_per_second': 1.751, 'epoch': 2.11}\n",
      "{'loss': 0.1052, 'learning_rate': 3.8416741294842757e-05, 'epoch': 2.32}\n",
      "{'loss': 0.0955, 'learning_rate': 3.7363693831322767e-05, 'epoch': 2.53}\n",
      "{'loss': 0.0953, 'learning_rate': 3.6310646367802777e-05, 'epoch': 2.74}\n",
      "{'loss': 0.0945, 'learning_rate': 3.525707211715746e-05, 'epoch': 2.95}\n",
      "{'loss': 0.0756, 'learning_rate': 3.420402465363747e-05, 'epoch': 3.16}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8013385aebee46899e1dab303dc4cb5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.505166053771973, 'eval_cer': 0.3093495696973245, 'eval_runtime': 2750.3964, 'eval_samples_per_second': 5.177, 'eval_steps_per_second': 1.726, 'epoch': 3.16}\n",
      "{'loss': 0.0792, 'learning_rate': 3.3151240583680136e-05, 'epoch': 3.37}\n",
      "{'loss': 0.0771, 'learning_rate': 3.209766633303482e-05, 'epoch': 3.58}\n",
      "{'loss': 0.0776, 'learning_rate': 3.104435547595217e-05, 'epoch': 3.79}\n",
      "{'loss': 0.0756, 'learning_rate': 2.9991308012432178e-05, 'epoch': 4.0}\n",
      "{'loss': 0.0563, 'learning_rate': 2.8937733761786863e-05, 'epoch': 4.21}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66cff606d7774797beab9c5834094d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.794046401977539, 'eval_cer': 0.11978221415607986, 'eval_runtime': 2690.793, 'eval_samples_per_second': 5.291, 'eval_steps_per_second': 1.764, 'epoch': 4.21}\n",
      "{'loss': 0.0581, 'learning_rate': 2.788468629826687e-05, 'epoch': 4.43}\n",
      "{'loss': 0.0609, 'learning_rate': 2.683137544118422e-05, 'epoch': 4.64}\n",
      "{'loss': 0.0608, 'learning_rate': 2.577832797766423e-05, 'epoch': 4.85}\n",
      "{'loss': 0.0492, 'learning_rate': 2.4725017120581572e-05, 'epoch': 5.06}\n",
      "{'loss': 0.0438, 'learning_rate': 2.367170626349892e-05, 'epoch': 5.27}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59bab49814674fe2b9bcd62f2b48fcf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.273472785949707, 'eval_cer': 0.229436215678239, 'eval_runtime': 2683.3841, 'eval_samples_per_second': 5.306, 'eval_steps_per_second': 1.769, 'epoch': 5.27}\n",
      "{'loss': 0.0448, 'learning_rate': 2.2618922193541592e-05, 'epoch': 5.48}\n",
      "{'loss': 0.0452, 'learning_rate': 2.1565347942896277e-05, 'epoch': 5.69}\n",
      "{'loss': 0.0451, 'learning_rate': 2.0511773692250963e-05, 'epoch': 5.9}\n",
      "{'loss': 0.0393, 'learning_rate': 1.9458989622293634e-05, 'epoch': 6.11}\n",
      "{'loss': 0.0317, 'learning_rate': 1.840567876521098e-05, 'epoch': 6.32}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4daa354f36524d06aa229eba6dd8bd96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.03128719329834, 'eval_cer': 0.3169603653181898, 'eval_runtime': 2671.6253, 'eval_samples_per_second': 5.329, 'eval_steps_per_second': 1.776, 'epoch': 6.32}\n",
      "{'loss': 0.0309, 'learning_rate': 1.7352367908128326e-05, 'epoch': 6.53}\n",
      "{'loss': 0.0297, 'learning_rate': 1.6299057051045675e-05, 'epoch': 6.74}\n",
      "{'loss': 0.0348, 'learning_rate': 1.524600958752568e-05, 'epoch': 6.95}\n",
      "{'loss': 0.0206, 'learning_rate': 1.4192435336880367e-05, 'epoch': 7.16}\n",
      "{'loss': 0.0236, 'learning_rate': 1.3139651266923036e-05, 'epoch': 7.38}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d009bf66284083ac79b279de576072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.167418479919434, 'eval_cer': 0.16017797552836485, 'eval_runtime': 2691.8952, 'eval_samples_per_second': 5.289, 'eval_steps_per_second': 1.763, 'epoch': 7.38}\n",
      "{'loss': 0.0197, 'learning_rate': 1.2086340409840384e-05, 'epoch': 7.59}\n",
      "{'loss': 0.0224, 'learning_rate': 1.1033292946320392e-05, 'epoch': 7.8}\n",
      "{'loss': 0.019, 'learning_rate': 9.97998208923774e-06, 'epoch': 8.01}\n",
      "{'loss': 0.0133, 'learning_rate': 8.926934625717748e-06, 'epoch': 8.22}\n",
      "{'loss': 0.0159, 'learning_rate': 7.873623768635094e-06, 'epoch': 8.43}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59c581c756e49d3ac0aba05db10ed2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.0792388916015625, 'eval_cer': 0.1537966161231778, 'eval_runtime': 2706.3699, 'eval_samples_per_second': 5.261, 'eval_steps_per_second': 1.754, 'epoch': 8.43}\n",
      "{'loss': 0.0156, 'learning_rate': 6.820312911552441e-06, 'epoch': 8.64}\n",
      "{'loss': 0.0107, 'learning_rate': 5.767002054469789e-06, 'epoch': 8.85}\n",
      "{'loss': 0.0109, 'learning_rate': 4.7134278038244746e-06, 'epoch': 9.06}\n",
      "{'loss': 0.0093, 'learning_rate': 3.660116946741822e-06, 'epoch': 9.27}\n",
      "{'loss': 0.0095, 'learning_rate': 2.6070694832218302e-06, 'epoch': 9.48}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67d5524763943fa81b4efee16fea976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.954314231872559, 'eval_cer': 0.11480592471166794, 'eval_runtime': 2692.7972, 'eval_samples_per_second': 5.287, 'eval_steps_per_second': 1.762, 'epoch': 9.48}\n",
      "{'loss': 0.0096, 'learning_rate': 1.5540220197018386e-06, 'epoch': 9.69}\n",
      "{'loss': 0.0044, 'learning_rate': 5.007111626191856e-07, 'epoch': 9.9}\n",
      "{'train_runtime': 102249.0808, 'train_samples_per_second': 5.57, 'train_steps_per_second': 1.857, 'train_loss': 0.0809435841758755, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Номер на футболке: 8\n"
     ]
    }
   ],
   "source": [
    "url = \"crops\\\\8\\\\ballerTV_137example.jpg\"\n",
    "image = Image.open(url).convert(\"RGB\")\n",
    "\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "generated_ids = model.generate(pixel_values.cuda())\n",
    "\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(f'Номер на футболке: {generated_text}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим eval_dataloader и Загрузим обученную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = DataLoader(eval_dataset, batch_size=32)\n",
    "model = VisionEncoderDecoderModel.from_pretrained('model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем точность обученной модели на eval датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the device to run the evaluation on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "model.to(device)\n",
    "# Evaluate the model on the eval dataset\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in tqdm(enumerate(eval_dataloader)):\n",
    "        #url = eval_df['file_name'][i]\n",
    "        target_text = processor.batch_decode(batch['labels'], skip_special_tokens=True)\n",
    "        #image = Image.open(url).convert(\"RGB\")        \n",
    "        pixel_values = batch['pixel_values']\n",
    "        generated_ids = model.generate(pixel_values.cuda())       \n",
    "\n",
    "        # Make a prediction\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        x = [] \n",
    "        for j in generated_text:       \n",
    "            if j.isdigit():\n",
    "                x.append(int(j))\n",
    "            else:\n",
    "                x.append(1000)        \n",
    "\n",
    "        # Save the true and predicted labels\n",
    "        y_true.append(target_text)\n",
    "        y_pred.append(x)  \n",
    "\n",
    "        if (i % 100  == 0) & (i > 2):\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            print(\"Accuracy:\", accuracy) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем точность обученной модели на eval датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df546972d964ea68dc2e5cf1236683b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9326683291770573\n",
      "Accuracy: 0.9450686641697877\n",
      "Accuracy: 0.9400499583680266\n",
      "Accuracy: 0.9394128669581512\n",
      "Accuracy: 0.9370314842578711\n",
      "Accuracy: 0.9354435651811746\n",
      "Accuracy: 0.9350232059978579\n",
      "Accuracy: 0.936582318025617\n",
      "Accuracy: 0.9358511524576506\n",
      "Accuracy: 0.9355161209697576\n",
      "Accuracy: 0.9363780958872984\n",
      "Accuracy: 0.9354301187252656\n",
      "Accuracy: 0.9363583926168044\n",
      "Accuracy: 0.93626138189609\n",
      "Accuracy: 0.9361773037827029\n",
      "Accuracy: 0.9361037337915951\n",
      "Accuracy: 0.9361858550213203\n",
      "Accuracy: 0.9352867657269823\n",
      "Accuracy: 0.934350743323247\n",
      "Accuracy: 0.9340082489688789\n",
      "Accuracy: 0.934174503035353\n",
      "Accuracy: 0.9331894102942847\n",
      "Accuracy: 0.9328333876752527\n",
      "Accuracy: 0.9336527445057806\n",
      "Accuracy: 0.9333066693330667\n",
      "Accuracy: 0.933275646572445\n",
      "Accuracy: 0.9331543375613369\n",
      "Accuracy: 0.9327738594768324\n",
      "Accuracy: 0.9321610206016723\n",
      "Accuracy: 0.9328389300891592\n",
      "Accuracy: 0.9329892750584631\n",
      "Accuracy: 0.9332864619951566\n",
      "Accuracy: 0.9331111279448526\n",
      "Accuracy: 0.932578486875965\n",
      "Accuracy: 0.9317905863866867\n",
      "Accuracy: 0.9317905863866867\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the device to run the evaluation on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "model.to(device)\n",
    "# Evaluate the model on the eval dataset\n",
    "y_true = []\n",
    "y_pred = []\n",
    "xx = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(eval_dataset)), nrows=2):\n",
    "        url = eval_df['file_name'][i]\n",
    "        xx.append(url)\n",
    "        target_text = int(eval_df['text'][i])\n",
    "        image = Image.open(url).convert(\"RGB\")   \n",
    "\n",
    "        pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "        generated_ids = model.generate(pixel_values.cuda(), max_new_tokens=3)\n",
    "\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        #pixel_values = batch['pixel_values']\n",
    "        #generated_ids = model.generate(pixel_values.cuda())       \n",
    "\n",
    "        # Make a prediction\n",
    "        #generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        x = []        \n",
    "        if generated_text.isdigit():\n",
    "            x.append(int(generated_text))\n",
    "        else:\n",
    "            x.append(1000)    \n",
    "\n",
    "        # Save the true and predicted labels\n",
    "        y_true.append(target_text)\n",
    "        y_pred.append(x)\n",
    "\n",
    "        if (i % 400  == 0) & (i > 2):\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            print(\"Accuracy:\", accuracy) \n",
    "\n",
    "print(\"Accuracy:\", accuracy)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
