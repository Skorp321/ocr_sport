{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, default_data_collator\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import albumentations as A\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завиксируем всю случайность!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 23\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.determenistic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем наш датасет из полученных файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150637, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basketball_df = pd.read_csv('for_train\\\\train_bascetball.csv')\n",
    "streetball_df = pd.read_csv('for_train\\\\train_streetball.csv')\n",
    "volleyball_df = pd.read_csv('for_train\\\\train_volleyball.csv')\n",
    "\n",
    "res_df = pd.concat([basketball_df, streetball_df, volleyball_df], axis=0)\n",
    "res_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем датасет на train test и eval части"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df, test_df, = train_test_split(res_df, test_size=0.2, shuffle=True)\n",
    "train_df, eval_df = train_test_split(diff_df, test_size=0.2)\n",
    "# we reset the indices to start from zero\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "eval_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.Perspective(),\n",
    "    A.Rotate(45),\n",
    "    A.Resize(height=40, width=40),\n",
    "    A.RandomCrop(width=32, height=32)   \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IAMDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=3, transforms = None):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      \n",
    "        file_name = self.df['file_name'][idx]\n",
    "        text = str(self.df['text'][idx])\n",
    "   \n",
    "        image = Image.open(file_name).convert(\"RGB\")\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image=np.array(image))[\"image\"]\n",
    "        \n",
    "        #image = image.resize((64, 64))\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        # add labels (input_ids) by encoding the text\n",
    "        labels = self.processor.tokenizer(text, \n",
    "                                          padding=\"max_length\", \n",
    "                                          max_length=self.max_target_length).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим наши датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "train_dataset = IAMDataset(root_dir='C:\\\\Users\\\\Mytre\\\\OneDrive\\\\Документы\\\\Data\\\\Work\\\\',\n",
    "                           df=train_df,\n",
    "                           processor=processor,\n",
    "                           transforms=transform)\n",
    "test_dataset = IAMDataset(root_dir='C:\\\\Users\\\\Mytre\\\\OneDrive\\\\Документы\\\\Data\\\\Work\\\\',\n",
    "                           df=test_df,\n",
    "                           processor=processor)\n",
    "eval_dataset = IAMDataset(root_dir='C:\\\\Users\\\\Mytre\\\\OneDrive\\\\Документы\\\\Data\\\\Work\\\\',\n",
    "                           df=eval_df,\n",
    "                           processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 96407\n",
      "Number of testing examples: 30128\n",
      "Number of validation examples: 24102\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of testing examples:\", len(test_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим предъобученный трансформер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#model = VisionEncoderDecoderModel.from_pretrained('model')\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сконфигурируем нашу модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 4\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сконфигурируем цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    num_train_epochs=40,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    fp16=True, \n",
    "    output_dir=\"G:\\\\models1\",\n",
    "    logging_steps=1000,\n",
    "    save_steps=10000,\n",
    "    eval_steps=10000,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оопределим метрику"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "acc_metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions    \n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)    \n",
    "\n",
    "    label_int = [int(x) for x in label_str]\n",
    "    x = [] \n",
    "    for j in pred_str:       \n",
    "        if j.isdigit():\n",
    "            x.append(int(j))\n",
    "        else:\n",
    "            x.append(1000)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    acc = acc_metric.compute(predictions=x, references=label_int)\n",
    "\n",
    "    return {\"cer\": cer,\n",
    "            \"accuracy\": acc}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переопределим оптимизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(),lr=0.00001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\.env\\lib\\site-packages\\transformers\\models\\trocr\\processing_trocr.py:134: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\.env\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59954fa30d446b48008ef93a0183e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1285440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3006, 'learning_rate': 4.9961336196166296e-05, 'epoch': 0.03}\n",
      "{'loss': 0.2464, 'learning_rate': 4.992243900921086e-05, 'epoch': 0.06}\n",
      "{'loss': 0.2321, 'learning_rate': 4.988354182225542e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2251, 'learning_rate': 4.984464463529997e-05, 'epoch': 0.12}\n",
      "{'loss': 0.1998, 'learning_rate': 4.980574744834454e-05, 'epoch': 0.16}\n",
      "{'loss': 0.2131, 'learning_rate': 4.97668502613891e-05, 'epoch': 0.19}\n",
      "{'loss': 0.2032, 'learning_rate': 4.9727991971620615e-05, 'epoch': 0.22}\n",
      "{'loss': 0.2137, 'learning_rate': 4.9689094784665175e-05, 'epoch': 0.25}\n",
      "{'loss': 0.1983, 'learning_rate': 4.9650197597709734e-05, 'epoch': 0.28}\n",
      "{'loss': 0.2073, 'learning_rate': 4.961137820512821e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ba720f41cf4aeebf1a9b61e4162c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.424245834350586, 'eval_cer': 0.1932088285229202, 'eval_accuracy': {'accuracy': 0.7943739108787653}, 'eval_runtime': 4547.2131, 'eval_samples_per_second': 5.3, 'eval_steps_per_second': 1.767, 'epoch': 0.31}\n",
      "{'loss': 0.178, 'learning_rate': 4.957248101817277e-05, 'epoch': 0.34}\n",
      "{'loss': 0.2035, 'learning_rate': 4.953358383121733e-05, 'epoch': 0.37}\n",
      "{'loss': 0.1783, 'learning_rate': 4.9494686644261887e-05, 'epoch': 0.4}\n",
      "{'loss': 0.1939, 'learning_rate': 4.945586725168036e-05, 'epoch': 0.44}\n",
      "{'loss': 0.1896, 'learning_rate': 4.941697006472492e-05, 'epoch': 0.47}\n",
      "{'loss': 0.1837, 'learning_rate': 4.937807287776948e-05, 'epoch': 0.5}\n",
      "{'loss': 0.1886, 'learning_rate': 4.9339175690814046e-05, 'epoch': 0.53}\n",
      "{'loss': 0.1926, 'learning_rate': 4.9300278503858605e-05, 'epoch': 0.56}\n",
      "{'loss': 0.1801, 'learning_rate': 4.926142021409012e-05, 'epoch': 0.59}\n",
      "{'loss': 0.1848, 'learning_rate': 4.922252302713468e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fca571e1044f68ba82c6b73f638cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.445227146148682, 'eval_cer': 0.1832937181663837, 'eval_accuracy': {'accuracy': 0.7944154012115178}, 'eval_runtime': 4354.262, 'eval_samples_per_second': 5.535, 'eval_steps_per_second': 1.845, 'epoch': 0.62}\n",
      "{'loss': 0.1788, 'learning_rate': 4.918362584017924e-05, 'epoch': 0.65}\n",
      "{'loss': 0.1806, 'learning_rate': 4.914476755041076e-05, 'epoch': 0.68}\n",
      "{'loss': 0.1766, 'learning_rate': 4.9105909260642275e-05, 'epoch': 0.72}\n",
      "{'loss': 0.1666, 'learning_rate': 4.9067012073686834e-05, 'epoch': 0.75}\n",
      "{'loss': 0.1575, 'learning_rate': 4.902815378391835e-05, 'epoch': 0.78}\n",
      "{'loss': 0.1908, 'learning_rate': 4.898929549414987e-05, 'epoch': 0.81}\n",
      "{'loss': 0.1885, 'learning_rate': 4.895039830719443e-05, 'epoch': 0.84}\n",
      "{'loss': 0.1699, 'learning_rate': 4.891150112023899e-05, 'epoch': 0.87}\n",
      "{'loss': 0.1918, 'learning_rate': 4.8872603933283546e-05, 'epoch': 0.9}\n",
      "{'loss': 0.1843, 'learning_rate': 4.8833706746328105e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ea3b840df54ad2a57b2541bfd84eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.519834518432617, 'eval_cer': 0.6140237691001698, 'eval_accuracy': {'accuracy': 0.366235167206041}, 'eval_runtime': 4455.1351, 'eval_samples_per_second': 5.41, 'eval_steps_per_second': 1.803, 'epoch': 0.93}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m      2\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m      3\u001b[0m     tokenizer\u001b[39m=\u001b[39mprocessor\u001b[39m.\u001b[39mfeature_extractor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     data_collator\u001b[39m=\u001b[39mdefault_data_collator,\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     12\u001b[0m model\u001b[39m.\u001b[39msave_pretrained(\u001b[39m'\u001b[39m\u001b[39mmodel_next\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\.env\\lib\\site-packages\\transformers\\trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1630\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1632\u001b[0m )\n\u001b[1;32m-> 1633\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1634\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1635\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1636\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1637\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1638\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\.env\\lib\\site-packages\\transformers\\trainer.py:1902\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1900\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1901\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1902\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1904\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1905\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1906\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1907\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1908\u001b[0m ):\n\u001b[0;32m   1909\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1910\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\.env\\lib\\site-packages\\transformers\\trainer.py:2655\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2652\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m   2654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_grad_scaling:\n\u001b[1;32m-> 2655\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m   2656\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_apex:\n\u001b[0;32m   2657\u001b[0m     \u001b[39mwith\u001b[39;00m amp\u001b[39m.\u001b[39mscale_loss(loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer) \u001b[39mas\u001b[39;00m scaled_loss:\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\.env\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\.env\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained('model_next')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"crops\\\\8\\\\ballerTV_137example.jpg\"\n",
    "image = Image.open(url).convert(\"RGB\")\n",
    "\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "generated_ids = model.generate(pixel_values.cuda())\n",
    "\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(f'Номер на футболке: {generated_text}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим eval_dataloader и Загрузим обученную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "model = VisionEncoderDecoderModel.from_pretrained('G:\\\\models1\\\\checkpoint-40000')\n",
    "\n",
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 4\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем точность обученной модели на eval датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679fd3c46d2b4d60a7dec9227fe3ba38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8768382352941176\n",
      "Accuracy: 0.880569306930693\n",
      "Accuracy: 0.8849337748344371\n",
      "Accuracy: 0.8851057213930348\n",
      "Accuracy: 0.8813496015936255\n",
      "Accuracy: 0.8822674418604651\n",
      "Accuracy: 0.8839921652421653\n",
      "Accuracy: 0.8827150872817955\n",
      "Accuracy: 0.8817904656319291\n",
      "Accuracy: 0.8832959081836327\n",
      "Accuracy: 0.8844714156079855\n",
      "Accuracy: 0.8843074043261231\n",
      "Accuracy: 0.8849366359447005\n",
      "Accuracy: 0.8859218972895863\n",
      "Accuracy: 0.8852363515312917\n",
      "Accuracy: 0.8845583645443196\n",
      "Accuracy: 0.8843639835487661\n",
      "Accuracy: 0.884225860155383\n",
      "Total accuracy: 0.8842887473460722\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# Define the device to run the evaluation on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "model.to(device)\n",
    "# Evaluate the model on the eval dataset\n",
    "diff_acc = []\n",
    "acc = []\n",
    "i = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader):\n",
    "    \n",
    "        target_text = processor.batch_decode(batch['labels'], skip_special_tokens=True)\n",
    "        target_text = [int(x) for x in target_text]\n",
    "           \n",
    "        pixel_values = batch['pixel_values']\n",
    "        generated_ids = model.generate(pixel_values.cuda(), max_new_tokens=4)       \n",
    "       \n",
    "        # Make a prediction\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        x = [] \n",
    "        for j in generated_text:       \n",
    "            if j.isdigit():\n",
    "                x.append(int(j))\n",
    "            else:\n",
    "                x.append(1000)        \n",
    "      \n",
    "        bach_acc = accuracy_score(target_text, x)\n",
    "        # Save the true and predicted labels\n",
    "        diff_acc.append(bach_acc)  \n",
    "        acc.append(bach_acc)\n",
    "\n",
    "        if (i % 50  == 0) & (i > 2):\n",
    "            accuracy = np.mean(diff_acc)\n",
    "            print(\"Accuracy:\", accuracy) \n",
    "        i += 1\n",
    "\n",
    "    print(f\"Total accuracy: {np.mean(acc)}\")    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем точность обученной модели на eval датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the device to run the evaluation on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "model.to(device)\n",
    "# Evaluate the model on the eval dataset\n",
    "y_true = []\n",
    "y_pred = []\n",
    "xx = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_dataset)), nrows=2):\n",
    "        url = test_df['file_name'][i]\n",
    "        xx.append(url)\n",
    "        target_text = int(test_df['text'][i])\n",
    "        image = Image.open(url).convert(\"RGB\")   \n",
    "\n",
    "        pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "        generated_ids = model.generate(pixel_values.cuda(), max_new_tokens=3)\n",
    "\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        #pixel_values = batch['pixel_values']\n",
    "        #generated_ids = model.generate(pixel_values.cuda())       \n",
    "\n",
    "        # Make a prediction\n",
    "        #generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        x = []        \n",
    "        if generated_text.isdigit():\n",
    "            \n",
    "            x = int(generated_text)\n",
    "        else:\n",
    "            x = 1000    \n",
    "        \n",
    "        # Save the true and predicted labels\n",
    "        y_true.append(target_text)\n",
    "        y_pred.append(x)\n",
    "\n",
    "        if (i % 400  == 0) & (i > 2):\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            print(\"Accuracy:\", accuracy) \n",
    "\n",
    "print(\"Accuracy:\", accuracy)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
