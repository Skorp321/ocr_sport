{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, default_data_collator\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завиксируем всю случайность!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 23\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.determenistic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем наш датасет из полученных файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150637, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basketball_df = pd.read_csv('for_train\\\\train_bascetball.csv')\n",
    "streetball_df = pd.read_csv('for_train\\\\train_streetball.csv')\n",
    "volleyball_df = pd.read_csv('for_train\\\\train_volleyball.csv')\n",
    "\n",
    "res_df = pd.concat([basketball_df, streetball_df, volleyball_df], axis=0)\n",
    "res_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем датасет на train test и eval части"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df, test_df, = train_test_split(res_df, test_size=0.2, shuffle=True)\n",
    "train_df, eval_df = train_test_split(diff_df, test_size=0.2)\n",
    "# we reset the indices to start from zero\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "eval_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_pathes = []\n",
    "\n",
    "class IAMDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=3):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      \n",
    "        file_name = self.df['file_name'][idx]\n",
    "        text = str(self.df['text'][idx])\n",
    "        store_pathes.append(file_name)\n",
    "   \n",
    "        image = Image.open(file_name).convert(\"RGB\")\n",
    "        image = image.resize((64, 64))\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        # add labels (input_ids) by encoding the text\n",
    "        labels = self.processor.tokenizer(text, \n",
    "                                          padding=\"max_length\", \n",
    "                                          max_length=self.max_target_length).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим наши датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "train_dataset = IAMDataset(root_dir='C:\\\\Users\\\\Mytre\\\\OneDrive\\\\Документы\\\\Data\\\\Work\\\\',\n",
    "                           df=train_df,\n",
    "                           processor=processor)\n",
    "test_dataset = IAMDataset(root_dir='C:\\\\Users\\\\Mytre\\\\OneDrive\\\\Документы\\\\Data\\\\Work\\\\',\n",
    "                           df=test_df,\n",
    "                           processor=processor)\n",
    "eval_dataset = IAMDataset(root_dir='C:\\\\Users\\\\Mytre\\\\OneDrive\\\\Документы\\\\Data\\\\Work\\\\',\n",
    "                           df=eval_df,\n",
    "                           processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 96407\n",
      "Number of testing examples: 30128\n",
      "Number of validation examples: 24102\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of testing examples:\", len(test_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '17', '']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.batch_decode(eval_dataset[0]['labels'], skip_special_tokens=True)\n",
    "#print(eval_dataset[0]['labels'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим предъобученный трансформер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-stage1 and are newly initialized: ['encoder.pooler.dense.weight', 'encoder.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\")\n",
    "#model = VisionEncoderDecoderModel.from_pretrained(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#model = VisionEncoderDecoderModel.from_pretrained('model')\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сконфигурируем нашу модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 4\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сконфигурируем цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    num_train_epochs=40,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    fp16=True, \n",
    "    output_dir=\"G:\\\\models1\",\n",
    "    logging_steps=1000,\n",
    "    save_steps=20000,\n",
    "    eval_steps=10000,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оопределим метрику"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "acc_metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def compute_metrics(pred):\n",
    "    label_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    #label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    #label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    x = [] \n",
    "    for j in pred_str:       \n",
    "        if j.isdigit():\n",
    "            x.append(int(j))\n",
    "        else:\n",
    "            x.append(1000)\n",
    "    label_ids = [int(x) for x in label_ids]       \n",
    "    acc = acc_metric.compute(predictions=x, references=label_ids)\n",
    "\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "def compute_cer(pred_ids, label_ids):\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return cer\n",
    "'''\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions    \n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)    \n",
    "\n",
    "    label_int = [int(x) for x in label_str]\n",
    "    x = [] \n",
    "    for j in pred_str:       \n",
    "        if j.isdigit():\n",
    "            x.append(int(j))\n",
    "        else:\n",
    "            x.append(1000)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    acc = acc_metric.compute(predictions=x, references=label_int)\n",
    "\n",
    "    return {\"cer\": cer,\n",
    "            \"accuracy\": acc}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переопределим оптимизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\.env\\lib\\site-packages\\transformers\\models\\trocr\\processing_trocr.py:134: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\.env\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00500ebc4ab945c0a5df554283ee3105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1285440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0801, 'learning_rate': 4.996118060741847e-05, 'epoch': 0.03}\n",
      "{'loss': 0.6178, 'learning_rate': 4.9922361214836946e-05, 'epoch': 0.06}\n",
      "{'loss': 0.4559, 'learning_rate': 4.9883502925068456e-05, 'epoch': 0.09}\n",
      "{'loss': 0.4198, 'learning_rate': 4.984460573811302e-05, 'epoch': 0.12}\n",
      "{'loss': 0.3682, 'learning_rate': 4.980574744834454e-05, 'epoch': 0.16}\n",
      "{'loss': 0.3815, 'learning_rate': 4.97668502613891e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3271, 'learning_rate': 4.9727991971620615e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3205, 'learning_rate': 4.9689094784665175e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3494, 'learning_rate': 4.9650197597709734e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3146, 'learning_rate': 4.961133930794125e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\.env\\lib\\site-packages\\transformers\\generation\\utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a2f33121104afc9e8fcae3ea0c64e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0118749141693115, 'eval_cer': 0.12200339558573854, 'eval_accuracy': {'accuracy': 0.8630404115841009}, 'eval_runtime': 4396.3491, 'eval_samples_per_second': 5.482, 'eval_steps_per_second': 1.827, 'epoch': 0.31}\n",
      "{'loss': 0.3125, 'learning_rate': 4.957244212098581e-05, 'epoch': 0.34}\n",
      "{'loss': 0.327, 'learning_rate': 4.953358383121733e-05, 'epoch': 0.37}\n",
      "{'loss': 0.2891, 'learning_rate': 4.9494686644261887e-05, 'epoch': 0.4}\n",
      "{'loss': 0.292, 'learning_rate': 4.9455828354493404e-05, 'epoch': 0.44}\n",
      "{'loss': 0.2584, 'learning_rate': 4.941693116753797e-05, 'epoch': 0.47}\n",
      "{'loss': 0.2721, 'learning_rate': 4.937803398058253e-05, 'epoch': 0.5}\n",
      "{'loss': 0.2675, 'learning_rate': 4.9339175690814046e-05, 'epoch': 0.53}\n",
      "{'loss': 0.2681, 'learning_rate': 4.9300317401045556e-05, 'epoch': 0.56}\n",
      "{'loss': 0.2192, 'learning_rate': 4.926142021409012e-05, 'epoch': 0.59}\n",
      "{'loss': 0.2566, 'learning_rate': 4.922256192432164e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a283d18bfd4e2ca459e9dfa4d6b1f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.727569580078125, 'eval_cer': 0.13534804753820034, 'eval_accuracy': {'accuracy': 0.8489751887810141}, 'eval_runtime': 4382.5075, 'eval_samples_per_second': 5.5, 'eval_steps_per_second': 1.833, 'epoch': 0.62}\n",
      "{'loss': 0.2282, 'learning_rate': 4.91836647373662e-05, 'epoch': 0.65}\n",
      "{'loss': 0.2116, 'learning_rate': 4.914476755041076e-05, 'epoch': 0.68}\n",
      "{'loss': 0.1978, 'learning_rate': 4.910587036345532e-05, 'epoch': 0.72}\n",
      "{'loss': 0.1799, 'learning_rate': 4.9067012073686834e-05, 'epoch': 0.75}\n",
      "{'loss': 0.1972, 'learning_rate': 4.902811488673139e-05, 'epoch': 0.78}\n",
      "{'loss': 0.1859, 'learning_rate': 4.898925659696291e-05, 'epoch': 0.81}\n",
      "{'loss': 0.1721, 'learning_rate': 4.895039830719443e-05, 'epoch': 0.84}\n",
      "{'loss': 0.1587, 'learning_rate': 4.891150112023899e-05, 'epoch': 0.87}\n",
      "{'loss': 0.1874, 'learning_rate': 4.8872642830470504e-05, 'epoch': 0.9}\n",
      "{'loss': 0.1704, 'learning_rate': 4.883374564351506e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d5da6850824486a2b8f1cb244c4037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5530920028686523, 'eval_cer': 0.13083191850594228, 'eval_accuracy': {'accuracy': 0.8560700356816862}, 'eval_runtime': 4398.885, 'eval_samples_per_second': 5.479, 'eval_steps_per_second': 1.826, 'epoch': 0.93}\n",
      "{'loss': 0.1683, 'learning_rate': 4.879484845655962e-05, 'epoch': 0.96}\n",
      "{'loss': 0.142, 'learning_rate': 4.875599016679114e-05, 'epoch': 1.0}\n",
      "{'loss': 0.1402, 'learning_rate': 4.87170929798357e-05, 'epoch': 1.03}\n",
      "{'loss': 0.1501, 'learning_rate': 4.867819579288026e-05, 'epoch': 1.06}\n",
      "{'loss': 0.1433, 'learning_rate': 4.863937640029873e-05, 'epoch': 1.09}\n",
      "{'loss': 0.148, 'learning_rate': 4.860047921334329e-05, 'epoch': 1.12}\n",
      "{'loss': 0.1426, 'learning_rate': 4.856158202638785e-05, 'epoch': 1.15}\n",
      "{'loss': 0.1603, 'learning_rate': 4.852268483943242e-05, 'epoch': 1.18}\n",
      "{'loss': 0.1215, 'learning_rate': 4.8483787652476977e-05, 'epoch': 1.21}\n",
      "{'loss': 0.1449, 'learning_rate': 4.8444929362708494e-05, 'epoch': 1.24}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584982df403f4a85be408271f6edd8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5393173694610596, 'eval_cer': 0.11089983022071308, 'eval_accuracy': {'accuracy': 0.8811301966641772}, 'eval_runtime': 4383.1655, 'eval_samples_per_second': 5.499, 'eval_steps_per_second': 1.833, 'epoch': 1.24}\n",
      "{'loss': 0.1444, 'learning_rate': 4.840603217575305e-05, 'epoch': 1.28}\n",
      "{'loss': 0.1492, 'learning_rate': 4.836713498879761e-05, 'epoch': 1.31}\n",
      "{'loss': 0.1329, 'learning_rate': 4.832823780184217e-05, 'epoch': 1.34}\n",
      "{'loss': 0.1365, 'learning_rate': 4.8289418409260646e-05, 'epoch': 1.37}\n",
      "{'loss': 0.1268, 'learning_rate': 4.8250521222305205e-05, 'epoch': 1.4}\n",
      "{'loss': 0.1127, 'learning_rate': 4.8211624035349765e-05, 'epoch': 1.43}\n",
      "{'loss': 0.148, 'learning_rate': 4.817276574558128e-05, 'epoch': 1.46}\n",
      "{'loss': 0.128, 'learning_rate': 4.813386855862584e-05, 'epoch': 1.49}\n",
      "{'loss': 0.146, 'learning_rate': 4.809497137167041e-05, 'epoch': 1.52}\n",
      "{'loss': 0.13, 'learning_rate': 4.8056113081901924e-05, 'epoch': 1.56}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be98d41015604f9ba9e8a87c619aef50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0631589889526367, 'eval_cer': 0.14210526315789473, 'eval_accuracy': {'accuracy': 0.8501784084308356}, 'eval_runtime': 4383.9752, 'eval_samples_per_second': 5.498, 'eval_steps_per_second': 1.833, 'epoch': 1.56}\n",
      "{'loss': 0.1143, 'learning_rate': 4.801721589494648e-05, 'epoch': 1.59}\n",
      "{'loss': 0.1443, 'learning_rate': 4.7978318707991036e-05, 'epoch': 1.62}\n",
      "{'loss': 0.1374, 'learning_rate': 4.79394215210356e-05, 'epoch': 1.65}\n",
      "{'loss': 0.125, 'learning_rate': 4.790056323126711e-05, 'epoch': 1.68}\n",
      "{'loss': 0.1211, 'learning_rate': 4.786166604431168e-05, 'epoch': 1.71}\n",
      "{'loss': 0.1161, 'learning_rate': 4.7822807754543195e-05, 'epoch': 1.74}\n",
      "{'loss': 0.1038, 'learning_rate': 4.7783910567587754e-05, 'epoch': 1.77}\n",
      "{'loss': 0.1328, 'learning_rate': 4.7745013380632314e-05, 'epoch': 1.8}\n",
      "{'loss': 0.1366, 'learning_rate': 4.770611619367687e-05, 'epoch': 1.84}\n",
      "{'loss': 0.1232, 'learning_rate': 4.766725790390839e-05, 'epoch': 1.87}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d40c3c6b6c4ebfa085428d5d7b4986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.8205153942108154, 'eval_cer': 0.30512733446519524, 'eval_accuracy': {'accuracy': 0.6618537880673803}, 'eval_runtime': 4379.3703, 'eval_samples_per_second': 5.504, 'eval_steps_per_second': 1.835, 'epoch': 1.87}\n",
      "{'loss': 0.1279, 'learning_rate': 4.762836071695295e-05, 'epoch': 1.9}\n",
      "{'loss': 0.1231, 'learning_rate': 4.7589463529997515e-05, 'epoch': 1.93}\n",
      "{'loss': 0.1196, 'learning_rate': 4.755060524022903e-05, 'epoch': 1.96}\n",
      "{'loss': 0.1281, 'learning_rate': 4.751170805327359e-05, 'epoch': 1.99}\n",
      "{'loss': 0.1025, 'learning_rate': 4.747281086631815e-05, 'epoch': 2.02}\n",
      "{'loss': 0.0949, 'learning_rate': 4.743395257654967e-05, 'epoch': 2.05}\n",
      "{'loss': 0.1135, 'learning_rate': 4.7395094286781185e-05, 'epoch': 2.08}\n",
      "{'loss': 0.1115, 'learning_rate': 4.7356197099825744e-05, 'epoch': 2.12}\n",
      "{'loss': 0.099, 'learning_rate': 4.73172999128703e-05, 'epoch': 2.15}\n",
      "{'loss': 0.1049, 'learning_rate': 4.727844162310182e-05, 'epoch': 2.18}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4013c472204ec68e04a216363dcd5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.5669026374816895, 'eval_cer': 0.36169779286926995, 'eval_accuracy': {'accuracy': 0.6326445938096423}, 'eval_runtime': 4387.1009, 'eval_samples_per_second': 5.494, 'eval_steps_per_second': 1.831, 'epoch': 2.18}\n",
      "{'loss': 0.1085, 'learning_rate': 4.723954443614638e-05, 'epoch': 2.21}\n",
      "{'loss': 0.1128, 'learning_rate': 4.72006861463779e-05, 'epoch': 2.24}\n",
      "{'loss': 0.1071, 'learning_rate': 4.716178895942246e-05, 'epoch': 2.27}\n",
      "{'loss': 0.1096, 'learning_rate': 4.7122891772467015e-05, 'epoch': 2.3}\n",
      "{'loss': 0.1069, 'learning_rate': 4.708403348269853e-05, 'epoch': 2.33}\n",
      "{'loss': 0.1031, 'learning_rate': 4.704513629574309e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0868, 'learning_rate': 4.700623910878765e-05, 'epoch': 2.4}\n",
      "{'loss': 0.1076, 'learning_rate': 4.696738081901917e-05, 'epoch': 2.43}\n",
      "{'loss': 0.099, 'learning_rate': 4.692848363206373e-05, 'epoch': 2.46}\n",
      "{'loss': 0.0998, 'learning_rate': 4.688958644510829e-05, 'epoch': 2.49}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d90de9042c1415b845b4e9db6e0bbe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.769071578979492, 'eval_cer': 0.4029881154499151, 'eval_accuracy': {'accuracy': 0.5733963986391171}, 'eval_runtime': 4464.3624, 'eval_samples_per_second': 5.399, 'eval_steps_per_second': 1.8, 'epoch': 2.49}\n",
      "{'loss': 0.0991, 'learning_rate': 4.685072815533981e-05, 'epoch': 2.52}\n",
      "{'loss': 0.1053, 'learning_rate': 4.681183096838437e-05, 'epoch': 2.55}\n",
      "{'loss': 0.1014, 'learning_rate': 4.6772972678615886e-05, 'epoch': 2.58}\n",
      "{'loss': 0.1067, 'learning_rate': 4.6734075491660446e-05, 'epoch': 2.61}\n",
      "{'loss': 0.1074, 'learning_rate': 4.669521720189196e-05, 'epoch': 2.65}\n",
      "{'loss': 0.0905, 'learning_rate': 4.665632001493652e-05, 'epoch': 2.68}\n",
      "{'loss': 0.1119, 'learning_rate': 4.661746172516804e-05, 'epoch': 2.71}\n",
      "{'loss': 0.0971, 'learning_rate': 4.65785645382126e-05, 'epoch': 2.74}\n",
      "{'loss': 0.119, 'learning_rate': 4.653966735125716e-05, 'epoch': 2.77}\n",
      "{'loss': 0.1019, 'learning_rate': 4.6500770164301723e-05, 'epoch': 2.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09022546e55f453c84e72a6111404492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.572350978851318, 'eval_cer': 0.3537181663837012, 'eval_accuracy': {'accuracy': 0.6240560949298813}, 'eval_runtime': 4531.9594, 'eval_samples_per_second': 5.318, 'eval_steps_per_second': 1.773, 'epoch': 2.8}\n",
      "{'loss': 0.1087, 'learning_rate': 4.6461911874533234e-05, 'epoch': 2.83}\n",
      "{'loss': 0.0965, 'learning_rate': 4.64230146875778e-05, 'epoch': 2.86}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m      2\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m      3\u001b[0m     tokenizer\u001b[39m=\u001b[39mprocessor\u001b[39m.\u001b[39mfeature_extractor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     data_collator\u001b[39m=\u001b[39mdefault_data_collator,\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     12\u001b[0m model\u001b[39m.\u001b[39msave_pretrained(\u001b[39m'\u001b[39m\u001b[39mmodel_next\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\.env\\lib\\site-packages\\transformers\\trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1630\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1632\u001b[0m )\n\u001b[1;32m-> 1633\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1634\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1635\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1636\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1637\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1638\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\.env\\lib\\site-packages\\transformers\\trainer.py:1964\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1962\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_grad_scaling:\n\u001b[0;32m   1963\u001b[0m     scale_before \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mget_scale()\n\u001b[1;32m-> 1964\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer)\n\u001b[0;32m   1965\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mupdate()\n\u001b[0;32m   1966\u001b[0m     scale_after \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mget_scale()\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\.env\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:341\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    339\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 341\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    343\u001b[0m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m OptState\u001b[39m.\u001b[39mSTEPPED\n\u001b[0;32m    345\u001b[0m \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\.env\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:288\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39msum\u001b[39m(v\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m--> 288\u001b[0m     retval \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mstep(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\.env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     67\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\.env\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Mytre\\OneDrive\\Документы\\Data\\Work\\.env\\lib\\site-packages\\transformers\\optimization.py:455\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    452\u001b[0m     bias_correction2 \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m beta2 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m state[\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    453\u001b[0m     step_size \u001b[39m=\u001b[39m step_size \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39msqrt(bias_correction2) \u001b[39m/\u001b[39m bias_correction1\n\u001b[1;32m--> 455\u001b[0m p\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n\u001b[0;32m    457\u001b[0m \u001b[39m# Just adding the square of the weights to the loss function is *not*\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[39m# the correct way of using L2 regularization/weight decay with Adam,\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[39m# since that will interact with the m and v parameters in strange ways.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39m# of the weights to the loss with plain (non-momentum) SGD.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[39m# Add weight decay at the end (fixed version)\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[39mif\u001b[39;00m group[\u001b[39m\"\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained('model_next')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"crops\\\\8\\\\ballerTV_137example.jpg\"\n",
    "image = Image.open(url).convert(\"RGB\")\n",
    "\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "generated_ids = model.generate(pixel_values.cuda())\n",
    "\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(f'Номер на футболке: {generated_text}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим eval_dataloader и Загрузим обученную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "model = VisionEncoderDecoderModel.from_pretrained('model')\n",
    "\n",
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_new_tokens = 4\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем точность обученной модели на eval датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# Define the device to run the evaluation on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "model.to(device)\n",
    "# Evaluate the model on the eval dataset\n",
    "diff_acc = []\n",
    "acc = []\n",
    "i = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader):\n",
    "    \n",
    "        target_text = processor.batch_decode(batch['labels'], skip_special_tokens=True)\n",
    "        target_text = [int(x) for x in target_text]\n",
    "           \n",
    "        pixel_values = batch['pixel_values']\n",
    "        generated_ids = model.generate(pixel_values.cuda(), max_new_tokens=4)       \n",
    "       \n",
    "        # Make a prediction\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        x = [] \n",
    "        for j in generated_text:       \n",
    "            if j.isdigit():\n",
    "                x.append(int(j))\n",
    "            else:\n",
    "                x.append(1000)        \n",
    "      \n",
    "        bach_acc = accuracy_score(target_text, x)\n",
    "        # Save the true and predicted labels\n",
    "        diff_acc.append(bach_acc)  \n",
    "        acc.append(bach_acc)\n",
    "\n",
    "        if (i % 50  == 0) & (i > 2):\n",
    "            accuracy = np.mean(diff_acc)\n",
    "            print(\"Accuracy:\", accuracy) \n",
    "        i += 1\n",
    "\n",
    "    print(f\"Total accuracy: {np.mean(acc)}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = pd.read_csv('anno_00new.csv')\n",
    "\n",
    "eval_dataset = IAMDataset(root_dir='C:\\\\Users\\\\Mytre\\\\OneDrive\\\\Документы\\\\Data\\\\Work\\\\',\n",
    "                           df=df10,\n",
    "                           processor=processor)\n",
    "\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем точность обученной модели на eval датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the device to run the evaluation on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "model.to(device)\n",
    "# Evaluate the model on the eval dataset\n",
    "y_true = []\n",
    "y_pred = []\n",
    "xx = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_dataset)), nrows=2):\n",
    "        url = test_df['file_name'][i]\n",
    "        xx.append(url)\n",
    "        target_text = int(test_df['text'][i])\n",
    "        image = Image.open(url).convert(\"RGB\")   \n",
    "\n",
    "        pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "        generated_ids = model.generate(pixel_values.cuda(), max_new_tokens=3)\n",
    "\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        #pixel_values = batch['pixel_values']\n",
    "        #generated_ids = model.generate(pixel_values.cuda())       \n",
    "\n",
    "        # Make a prediction\n",
    "        #generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        x = []        \n",
    "        if generated_text.isdigit():\n",
    "            \n",
    "            x = int(generated_text)\n",
    "        else:\n",
    "            x = 1000    \n",
    "        \n",
    "        # Save the true and predicted labels\n",
    "        y_true.append(target_text)\n",
    "        y_pred.append(x)\n",
    "\n",
    "        if (i % 400  == 0) & (i > 2):\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            print(\"Accuracy:\", accuracy) \n",
    "\n",
    "print(\"Accuracy:\", accuracy)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
